{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888d70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optional) Install dependencies — uncomment to run in a fresh environment / Colab\n",
    "# !pip install tensorflow\n",
    "# !pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "749fa447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 199 characters, vocab size: 25\n",
      "Number of training sequences: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">78,848</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,225</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m78,848\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m3,225\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,073</span> (320.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m82,073\u001b[0m (320.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,073</span> (320.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m82,073\u001b[0m (320.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 3.1969 \n",
      "Epoch 2/6\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.7512\n",
      "Epoch 3/6\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2574\n",
      "Epoch 4/6\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.8743\n",
      "Epoch 5/6\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.9323\n",
      "Epoch 6/6\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.8469\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = \"\"\"Artificial intelligence helps with writing, coding and creativity.\n",
    "Text generation models learn patterns from examples and can produce coherent paragraphs.\n",
    "This tiny corpus is only for demo purposes.\"\"\".lower()\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(list(set(corpus)))\n",
    "char_to_idx = {c:i for i,c in enumerate(chars)}\n",
    "idx_to_char = {i:c for c,i in char_to_idx.items()}\n",
    "\n",
    "print(f\"Corpus length: {len(corpus)} characters, vocab size: {len(chars)}\")\n",
    "\n",
    "\n",
    "# Sequence settings\n",
    "seq_length = 40  # keep small for demo\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(corpus) - seq_length, step):\n",
    "    sentences.append(corpus[i: i + seq_length])\n",
    "    next_chars.append(corpus[i + seq_length])\n",
    "\n",
    "print(f\"Number of training sequences: {len(sentences)}\")\n",
    "\n",
    "\n",
    "# One-hot encoding (small)\n",
    "import numpy as np\n",
    "X = np.zeros((len(sentences), seq_length, len(chars)), dtype=np.bool_)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool_)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, ch in enumerate(sentence):\n",
    "        X[i, t, char_to_idx[ch]] = 1\n",
    "    y[i, char_to_idx[next_chars[i]]] = 1\n",
    "\n",
    "# Model definition (Keras)\n",
    "# NOTE: This cell requires tensorflow installed to run.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.01))\n",
    "model.summary()\n",
    "\n",
    "# Train for a tiny number of epochs so it's fast in demos\n",
    "epochs = 6  # increase for better results\n",
    "history = model.fit(X, y, batch_size=16, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f3dcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial intelligence helps with writircnnrenpp le pea tes  cedem ep.nrmeefcgddpeeereamicc otcdep eptmeacnacdpycepepeepelr adceoaefcerrpanp eecac colpersreodreor ,aemxeeeecmdsenceeeepedteeredepnesr.tlclmn  eecnnceeeepallevrmerert aeepedeee.lcierep eiepdgepen vetgeierfeeess re pv edcregenm eepr eeaeeeceleadelesmgrca nrehppeeedreerececeeldettpvaevedagacerarereeen anaeccpsrrceme ec teeetedsarleane el ea.eos eeegire.ecprddecpcme  rectec e\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(model, seed, length=300, temperature=1.0):\n",
    "    generated = seed\n",
    "    sentence = seed[-seq_length:]\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, seq_length, len(chars)))\n",
    "        for t, ch in enumerate(sentence):\n",
    "            if ch in char_to_idx:\n",
    "                x_pred[0, t, char_to_idx[ch]] = 1.\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = idx_to_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "seed = corpus[:seq_length]\n",
    "try:\n",
    "    print(generate_text(model, seed, length=400, temperature=0.8))\n",
    "except NameError:\n",
    "    print(\"Run the training cell first to build 'model' and train it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b4e2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a code skeleton. Uncomment the API call after installing the 'openai' package and setting your OPENAI_API_KEY environment variable.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example GPT usage (skeleton) — requires your OpenAI API key and the 'openai' package.\n",
    "# Do NOT store API keys in notebooks in plaintext for production — use environment variables or secrets.\n",
    "\n",
    "# pip install openai\n",
    "# export OPENAI_API_KEY='your_api_key_here'  # or set in your environment\n",
    "\n",
    "import os\n",
    "# import openai\n",
    "# openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Example prompt and call (uncomment after installing openai and setting API key)\n",
    "# response = openai.ChatCompletion.create(\n",
    "#     model='gpt-4o-mini',  # or another model accessible to you\n",
    "#     messages=[\n",
    "#         {'role': 'system', 'content': 'You are a helpful assistant that writes coherent paragraphs.'},\n",
    "#         {'role': 'user', 'content': 'Write a 3-paragraph overview of renewable energy advantages.'}\n",
    "#     ],\n",
    "#     max_tokens=300,\n",
    "#     temperature=0.7,\n",
    "# )\n",
    "# text = response['choices'][0]['message']['content']\n",
    "# print(text)\n",
    "\n",
    "print(\"This is a code skeleton. Uncomment the API call after installing the 'openai' package and setting your OPENAI_API_KEY environment variable.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
